# -*- coding: utf-8 -*-
"""online_anomaly_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t-SwzQ9VoX80bNqQPhXpvndI-jX63lgq

### Clona repositórito do Github
nota: adicionar login e usuário porque o repositório é privado
"""

import os
from getpass import getpass

import pandas as pd
import numpy as np
np.seterr(divide='ignore', invalid='ignore')
from gensim.models import Word2Vec
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

class Case:
    activities = []
    label = ''

    def __init__(self, id):
        self.id = str(id)

    def __repr__(self):
        return str(self.id)

    def updateTrace(self, activity_name):
        self.activities.append(activity_name)

    def updateLabel(self, label):
        self.label = label


class DataCloud:
    N = 0

    def __init__(self, x):
        self.n = 1
        self.mean = x
        self.variance = 0
        self.pertinency = 1
        DataCloud.N += 1

    def addDataClaud(self, x):
        self.n = 2
        self.mean = (self.mean + x) / 2
        self.variance = ((np.linalg.norm(self.mean - x)) ** 2)

    def updateDataCloud(self, n, mean, variance):
        self.n = n
        self.mean = mean
        self.variance = variance


class AutoCloud:
    c = np.array([DataCloud(0)], dtype=DataCloud)
    alfa = np.array([0.0], dtype=float)
    intersection = np.zeros((1, 1), dtype=int)
    listIntersection = np.zeros((1), dtype=int)
    matrixIntersection = np.zeros((1, 1), dtype=int)
    relevanceList = np.zeros((1), dtype=int)
    k = 1

    def __init__(self, m):
        AutoCloud.m = m
        AutoCloud.c = np.array([DataCloud(0)], dtype=DataCloud)
        AutoCloud.alfa = np.array([0.0], dtype=float)
        AutoCloud.intersection = np.zeros((1, 1), dtype=int)
        AutoCloud.listIntersection = np.zeros((1), dtype=int)
        AutoCloud.relevanceList = np.zeros((1), dtype=int)
        AutoCloud.matrixIntersection = np.zeros((1, 1), dtype=int)
        AutoCloud.k = 1
        AutoCloud.classIndex = [[1], [1]]

    def plotgrafico(self, x, y, index):
        for data in y:
            plt.plot(x, data, '.', color=listaCor[index])

    def mergeClouds(self):
        i = 0
        while (i < len(AutoCloud.listIntersection) - 1):
            merge = False
            j = i + 1
            while (j < len(AutoCloud.listIntersection)):
                # print("i",i,"j",j,"l",np.size(AutoCloud.listIntersection),"m",np.size(AutoCloud.matrixIntersection),"c",np.size(AutoCloud.c))
                if (AutoCloud.listIntersection[i] == 1 and AutoCloud.listIntersection[j] == 1):
                    AutoCloud.matrixIntersection[i, j] = AutoCloud.matrixIntersection[i, j] + 1;
                nI = AutoCloud.c[i].n
                nJ = AutoCloud.c[j].n
                meanI = AutoCloud.c[i].mean
                meanJ = AutoCloud.c[j].mean
                varianceI = AutoCloud.c[i].variance
                varianceJ = AutoCloud.c[j].variance
                nIntersc = AutoCloud.matrixIntersection[i, j]
                if (nIntersc > (nI - nIntersc) or nIntersc > (nJ - nIntersc)):
                    merge = True
                    # update values
                    n = nI + nJ - nIntersc
                    mean = ((nI * meanI) + (nJ * meanJ)) / (nI + nJ)
                    variance = ((nI - 1) * varianceI + (nJ - 1) * varianceJ) / (nI + nJ - 2)
                    newCloud = DataCloud(mean)
                    newCloud.updateDataCloud(n, mean, variance)
                    # atualizando lista de interseção
                    AutoCloud.listIntersection = np.concatenate((AutoCloud.listIntersection[0: i], np.array([1]),
                                                                 AutoCloud.listIntersection[i + 1: j],
                                                                 AutoCloud.listIntersection[
                                                                 j + 1: np.size(AutoCloud.listIntersection)]),
                                                                axis=None)
                    # atualizando lista de data clouds
                    AutoCloud.c = np.concatenate((AutoCloud.c[0: i], np.array([newCloud]), AutoCloud.c[i + 1: j],
                                                  AutoCloud.c[j + 1: np.size(AutoCloud.c)]), axis=None)
                    # update  intersection matrix
                    M0 = AutoCloud.matrixIntersection
                    # Remover linhas
                    M1 = np.concatenate((M0[0: i, :], np.zeros((1, len(M0))), M0[i + 1: j, :], M0[j + 1: len(M0), :]))
                    # remover colunas
                    M1 = np.concatenate((M1[:, 0: i], np.zeros((len(M1), 1)), M1[:, i + 1: j], M1[:, j + 1: len(M0)]),
                                        axis=1)
                    # calculando nova coluna
                    col = (M0[:, i] + M0[:, j]) * (M0[:, i] * M0[:, j] != 0)
                    col = np.concatenate((col[0: j], col[j + 1: np.size(col)]))
                    # calculando nova linha
                    lin = (M0[i, :] + M0[j, :]) * (M0[i, :] * M0[j, :] != 0)
                    lin = np.concatenate((lin[0: j], lin[j + 1: np.size(lin)]))
                    # atualizando coluna
                    M1[:, i] = col
                    # atualizando linha
                    M1[i, :] = lin
                    M1[i, i + 1: j] = M0[i, i + 1: j] + M0[i + 1: j, j].T;
                    AutoCloud.matrixIntersection = M1
                j += 1
            if (merge):
                i = 0
            else:
                i += 1

    def run(self, X):
        AutoCloud.listIntersection = np.zeros((np.size(AutoCloud.c)), dtype=int)
        if AutoCloud.k == 1:
            AutoCloud.c[0] = DataCloud(X)

        elif AutoCloud.k == 2:
            AutoCloud.c[0].addDataClaud(X)
        elif AutoCloud.k >= 3:
            i = 0
            createCloud = True
            AutoCloud.alfa = np.zeros((np.size(AutoCloud.c)), dtype=float)
            for data in AutoCloud.c:
                n = data.n + 1
                mean = ((n - 1) / n) * data.mean + (1 / n) * X
                variance = ((n - 1) / n) * data.variance + (1 / n) * ((np.linalg.norm(X - mean)) ** 2)
                eccentricity = (1 / n) + ((mean - X).T.dot(mean - X)) / (n * variance)
                typicality = 1 - eccentricity
                norm_eccentricity = eccentricity / 2
                norm_typicality = typicality / (AutoCloud.k - 2)

                #if norm_eccentricity > (AutoCloud.m ** 2 + 1) / (2 * n):
                #    print('True')
                #else:
                #    print('False')

                if (norm_eccentricity <= (AutoCloud.m ** 2 + 1) / (2 * n)):
                    data.updateDataCloud(n, mean, variance)
                    AutoCloud.alfa[i] = norm_typicality
                    createCloud = False
                    AutoCloud.listIntersection.itemset(i, 1)
                else:
                    AutoCloud.alfa[i] = norm_typicality
                    AutoCloud.listIntersection.itemset(i, 0)
                i += 1

            if (createCloud):
                AutoCloud.c = np.append(AutoCloud.c, DataCloud(X))
                AutoCloud.listIntersection = np.insert(AutoCloud.listIntersection, i, 1)
                AutoCloud.matrixIntersection = np.pad(AutoCloud.matrixIntersection, ((0, 1), (0, 1)), 'constant',
                                                      constant_values=(0))
            self.mergeClouds()
            AutoCloud.relevanceList = AutoCloud.alfa / np.sum(AutoCloud.alfa)
            AutoCloud.classIndex.append(np.argmax(AutoCloud.relevanceList))
            AutoCloud.classIndex.append(AutoCloud.alfa)

        AutoCloud.k = AutoCloud.k + 1


def read_log(path, log):
    '''
    Reads and preprocesses event log
    '''
    df_raw = pd.read_csv(f'{path}/{log}')
    df_raw['activity'] = df_raw['activity_name'].str.replace(' ', '-')
    df_proc = df_raw[['case_id', 'activity', 'label']]
    del df_raw

    return df_proc


def cases_y_list(df):
    '''
    Creates a list of cases (and their respective labels) for model training
    '''
    cases, case_id = [], []
    for group in df.groupby('case_id'):
        case = Case(group[0])
        case.activities = list(group[1].activity)
        case.label = list(group[1].label)[0]
        cases.append(case)
        case_id.append(group[0])

    return cases, case_id


def create_model(cases, dimensions, window, min_count):
    '''
    Creates a word2vec model
    '''
    print('Creates a word2vec model.')
    model = Word2Vec(
        size=dimensions,
        window=window,
        min_count=min_count,
        workers=-1)
    model.build_vocab(cases)
    model.train(cases, total_examples=len(cases), epochs=10)
    model.wv.vocab
    model.save("current_model")

    return model


def update_model(cases):
    '''
    TODO
    Updates word2vec model
    '''
    print('Updates a word2vec model.')
    new_model = Word2Vec.load("current_model")
    new_model.build_vocab(cases, update=True)
    new_model.train(cases, total_examples=2, epochs=1)
    new_model.wv.vocab

    return new_model


def average_case_vector(trace, model):
    '''
    Computes the average case feature vector according to a model
    '''
    case_vector = []
    for token in trace:
        try:
            case_vector.append(model.wv[token])
        except KeyError:
            pass
    return np.array(case_vector).mean(axis=0)


def average_vectors(traces, model):
    '''
    Computes average feature vector for several cases
    '''
    vectors = []
    for trace in traces:
        vectors.append(average_case_vector(trace, model))

    return vectors


def clean_case_memory(cases_in_memory):
    '''
    TODO
    Deletes older cases
    '''
    cases = cases_in_memory
    return cases


"""### Inicialização da base"""

path = 'data/'
log = 'sample_data.csv'

df = read_log(path, log)

"""### Criação do modelo word2vec
O número de eventos usados para criação do modelo é definido a partir da variável `stream_window` (tamanho da janela). Com isso, os eventos são agrupados em cases e o modelo é inicializado. Aqui define-se também os parâmetros do word2vec.
"""

stream_window = 1000
stream_windowed = 0
dimensions_word2vec = 50
window_word2vec = 3
minimum_word2vec = 3

case_ids = []

df_train = df.iloc[:stream_window]
cases_in_memory, case_ids = cases_y_list(df_train)

train_word2vec = []
for case in cases_in_memory:
    train_word2vec.append(case.activities)

word2vec_model = create_model(train_word2vec, dimensions_word2vec, window_word2vec, minimum_word2vec)

"""### Inicialização do AutoCloud
Calcula-se os vetores médios dos cases utilizados para criação do modelo word2vec. Após isso, o AutoCloud é inicializado e alimentado com os vetores iniciais.
"""

m = 1.7
auto_cloud = AutoCloud(m)
vector = np.ndarray([])

vectors = average_vectors(train_word2vec, word2vec_model)
for vector in vectors:
    auto_cloud.run(vector)

"""### Processamento da stream de eventos
Aqui iteramos pelo dataframe que contém os eventos (simulando uma stream). Para cada evento, verificamos se seu case já existe na lista de cases e atualizamos a lista.

**TODO**:
* Atualizar lista de cases a partir do janelamento
* Atualizar modelo word2vec (treinar com os cases mais recentes)
* Calcular métricas da clusterização
"""

stream_windowed = stream_window

for event in df.iloc[stream_window:].values:

    if next((case for case in cases_in_memory if case.id == str(event[0])), None):
        '''
        Case já existe. Portanto, atualizamos o valor dele na lista de cases
        '''
        case.updateTrace(event[1])
    else:
        '''
        Case não existe. Portanto, criamos um novo
        '''
        case = Case(event[0])
        case.activities = list(event[1])
        case.label = event[2]
        cases_in_memory.append(case)
        case_ids.append(case)

    vector = average_case_vector(case.activities, word2vec_model)
    auto_cloud.run(vector)

    if np.size(case_ids) == stream_windowed:
        stream_windowed += stream_window
        train_word2vec = []
        for case in cases_in_memory:
            train_word2vec.append(case.activities)
            word2vec_model = update_model(train_word2vec)


    # if condicao_janela (stream_window):
    #     cases_in_memory = clean_case_memory(cases_in_memory)
    #     word2vec_model = update_model(selected_cases, word2vec_model)

print(auto_cloud.alfa)
print(vector)
print(np.size(auto_cloud.c))
print(vector)
print(vector.T)
